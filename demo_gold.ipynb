{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Downloading polars-1.12.0-cp39-abi3-win_amd64.whl.metadata (14 kB)\n",
      "Downloading polars-1.12.0-cp39-abi3-win_amd64.whl (33.8 MB)\n",
      "   ---------------------------------------- 0.0/33.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/33.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/33.8 MB 4.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.9/33.8 MB 9.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 7.1/33.8 MB 10.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 10.5/33.8 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 13.9/33.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 16.8/33.8 MB 12.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 17.0/33.8 MB 12.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 22.8/33.8 MB 13.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 26.0/33.8 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 29.4/33.8 MB 13.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 32.5/33.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  33.6/33.8 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 33.8/33.8 MB 12.8 MB/s eta 0:00:00\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./airflow')\n",
    "\n",
    "import polars as pl\n",
    "import pymongo\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from core.config import get_settings\n",
    "from functions.operators.database import MongoDBOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = get_settings()\n",
    "mongo_operator = MongoDBOperator('imcp', settings.DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def tokenize(text: str) -> str:\n",
    "    if text is None:\n",
    "        return [\"\"]\n",
    "    return text.split(\" \")\n",
    "\n",
    "def scaling_data(df:pl.DataFrame, selected_columns:list=None):\n",
    "    if selected_columns != None:\n",
    "        temp_df = df.select(selected_columns)\n",
    "    else:\n",
    "        temp_df = df.select('*')\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size:int=10000, limit:int=100000):\n",
    "    with pymongo.MongoClient(settings.DATABASE_URL) as client:\n",
    "        db = client['imcp']\n",
    "        documents = db['huggingface'].find({}).batch_size(batch_size).limit(limit)\n",
    "        batch = []\n",
    "        for doc in documents:\n",
    "            batch.append(doc)\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch  # Trả về nhóm tài liệu (batch)\n",
    "                batch = []  # Reset batch sau khi yield\n",
    "        # Nếu còn tài liệu dư ra sau khi lặp xong\n",
    "        if batch:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n",
      "SUCCESS with 10000\n"
     ]
    }
   ],
   "source": [
    "for batch in data_generator():\n",
    "    data = list(batch)\n",
    "    df = pl.DataFrame(data).drop('_id')\n",
    "    lowered_df = df.with_columns(\n",
    "        *[pl.col(col).str.to_lowercase().alias(col) for col in ['caption','short_caption']]\n",
    "    )\n",
    "    cleaned_df = lowered_df.with_columns(\n",
    "       *[ pl.col(col).map_elements(lambda x: clean_text(x), return_dtype=pl.String).alias(col) for col in ['caption','short_caption']]\n",
    "    )\n",
    "    tokenized_df = cleaned_df.with_columns(\n",
    "        *[ pl.col(col).map_elements(lambda x: tokenize(x), return_dtype=pl.List(pl.String)).alias(f'{col}_tokens') for col in ['caption','short_caption']]\n",
    "    )\n",
    "    refined_df = scaling_data(tokenized_df, ['url', 'caption', 'short_caption', 'caption_tokens', 'short_caption_tokens', 'publisher', 'created_time'])\n",
    "    data = refined_df.to_dicts()\n",
    "    mongo_operator.insert('refined', data)\n",
    "    print('SUCCESS with', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217868, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame(data).drop('_id')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import minio\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # with open(\"./airflow/data/extracted_features.pkl\", \"rb\") as file:\n",
    "    #     data = pickle.load(file)\n",
    "    #     print(len(data.keys()))\n",
    "    #     print(np.array(data['http://images.cocodataset.org/val2017/000000400573.jpg']).shape)\n",
    "        \n",
    "        \n",
    "    client = minio.Minio(\n",
    "        \"116.118.50.253:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    objects = client.list_objects(\n",
    "        \"mlflow\",\n",
    "        prefix=\"raw_data/raw_images\",\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    for obj in objects:\n",
    "        count += 1\n",
    "    print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
